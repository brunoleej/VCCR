{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Import packages"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fba5b45bbe1dbd30"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install -q numpy\n",
    "!pip install -q matplotlib\n",
    "!pip install -q mujoco\n",
    "!command -v ffmpeg >/dev/null || (apt update && apt install -y ffmpeg)\n",
    "!pip install -q mediapy\n",
    "\n",
    "%env MUJOCO_GL=egl"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "10ed768650480178"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import mediapy as media\n",
    "import matplotlib.pyplot as plt\n",
    "import mujoco\n",
    "from copy import deepcopy\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import scipy.signal\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.distributions.normal import Normal\n",
    "from torch.distributions import kl_divergence\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7c984ee47a469450"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Define MuJoCo Environment"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "de07865aff729a0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "xml_string=\"\"\"<!-- Cheetah Model\n",
    "\n",
    "    The state space is populated with joints in the order that they are\n",
    "    defined in this file. The actuators also operate on joints.\n",
    "\n",
    "    State-Space (name/joint/parameter):\n",
    "        - rootx     slider      position (m)\n",
    "        - rootz     slider      position (m)\n",
    "        - rooty     hinge       angle (rad)\n",
    "        - bthigh    hinge       angle (rad)\n",
    "        - bshin     hinge       angle (rad)\n",
    "        - bfoot     hinge       angle (rad)\n",
    "        - fthigh    hinge       angle (rad)\n",
    "        - fshin     hinge       angle (rad)\n",
    "        - ffoot     hinge       angle (rad)\n",
    "        - rootx     slider      velocity (m/s)\n",
    "        - rootz     slider      velocity (m/s)\n",
    "        - rooty     hinge       angular velocity (rad/s)\n",
    "        - bthigh    hinge       angular velocity (rad/s)\n",
    "        - bshin     hinge       angular velocity (rad/s)\n",
    "        - bfoot     hinge       angular velocity (rad/s)\n",
    "        - fthigh    hinge       angular velocity (rad/s)\n",
    "        - fshin     hinge       angular velocity (rad/s)\n",
    "        - ffoot     hinge       angular velocity (rad/s)\n",
    "\n",
    "    Actuators (name/actuator/parameter):\n",
    "        - bthigh    hinge       torque (N m)\n",
    "        - bshin     hinge       torque (N m)\n",
    "        - bfoot     hinge       torque (N m)\n",
    "        - fthigh    hinge       torque (N m)\n",
    "        - fshin     hinge       torque (N m)\n",
    "        - ffoot     hinge       torque (N m)\n",
    "\n",
    "-->\n",
    "<mujoco model=\"cheetah\">\n",
    "  <compiler angle=\"radian\" coordinate=\"local\" inertiafromgeom=\"true\" settotalmass=\"14\"/>\n",
    "  <default>\n",
    "    <joint armature=\".1\" damping=\".01\" limited=\"true\" solimplimit=\"0 .8 .03\" solreflimit=\".02 1\" stiffness=\"8\"/>\n",
    "    <geom conaffinity=\"0\" condim=\"3\" contype=\"1\" friction=\".4 .1 .1\" rgba=\"0.8 0.6 .4 1\" solimp=\"0.0 0.8 0.01\" solref=\"0.02 1\"/>\n",
    "    <motor ctrllimited=\"true\" ctrlrange=\"-1 1\"/>\n",
    "  </default>\n",
    "  <size nstack=\"300000\" nuser_geom=\"1\"/>\n",
    "  <option gravity=\"0 0 -9.81\" timestep=\"0.01\"/>\n",
    "  <asset>\n",
    "    <texture builtin=\"gradient\" height=\"100\" rgb1=\"1 1 1\" rgb2=\"0 0 0\" type=\"skybox\" width=\"100\"/>\n",
    "    <texture builtin=\"flat\" height=\"1278\" mark=\"cross\" markrgb=\"1 1 1\" name=\"texgeom\" random=\"0.01\" rgb1=\"0.8 0.6 0.4\" rgb2=\"0.8 0.6 0.4\" type=\"cube\" width=\"127\"/>\n",
    "    <texture builtin=\"checker\" height=\"100\" name=\"texplane\" rgb1=\"0 0 0\" rgb2=\"0.8 0.8 0.8\" type=\"2d\" width=\"100\"/>\n",
    "    <material name=\"MatPlane\" reflectance=\"0.5\" shininess=\"1\" specular=\"1\" texrepeat=\"60 60\" texture=\"texplane\"/>\n",
    "    <material name=\"geom\" texture=\"texgeom\" texuniform=\"true\"/>\n",
    "  </asset>\n",
    "  <worldbody>\n",
    "    <light cutoff=\"100\" diffuse=\"1 1 1\" dir=\"-0 0 -1.3\" directional=\"true\" exponent=\"1\" pos=\"0 0 1.3\" specular=\".1 .1 .1\"/>\n",
    "    <geom conaffinity=\"1\" condim=\"3\" material=\"MatPlane\" name=\"floor\" pos=\"0 0 0\" rgba=\"0.8 0.9 0.8 1\" size=\"40 40 40\" type=\"plane\"/>\n",
    "    <body name=\"torso\" pos=\"0 0 .7\">\n",
    "      <camera name=\"track\" mode=\"trackcom\" pos=\"0 -3 0.3\" xyaxes=\"1 0 0 0 0 1\"/>\n",
    "      <joint armature=\"0\" axis=\"1 0 0\" damping=\"0\" limited=\"false\" name=\"rootx\" pos=\"0 0 0\" stiffness=\"0\" type=\"slide\"/>\n",
    "      <joint armature=\"0\" axis=\"0 0 1\" damping=\"0\" limited=\"false\" name=\"rootz\" pos=\"0 0 0\" stiffness=\"0\" type=\"slide\"/>\n",
    "      <joint armature=\"0\" axis=\"0 1 0\" damping=\"0\" limited=\"false\" name=\"rooty\" pos=\"0 0 0\" stiffness=\"0\" type=\"hinge\"/>\n",
    "      <geom fromto=\"-.5 0 0 .5 0 0\" name=\"torso\" size=\"0.046\" type=\"capsule\"/>\n",
    "      <geom axisangle=\"0 1 0 .87\" name=\"head\" pos=\".6 0 .1\" size=\"0.046 .15\" type=\"capsule\"/>\n",
    "      <!-- <site name='tip'  pos='.15 0 .11'/>-->\n",
    "      <body name=\"bthigh\" pos=\"-.5 0 0\">\n",
    "        <joint axis=\"0 1 0\" damping=\"6\" name=\"bthigh\" pos=\"0 0 0\" range=\"-.52 1.05\" stiffness=\"240\" type=\"hinge\"/>\n",
    "        <geom axisangle=\"0 1 0 -3.8\" name=\"bthigh\" pos=\".1 0 -.13\" size=\"0.046 .145\" type=\"capsule\"/>\n",
    "        <body name=\"bshin\" pos=\".16 0 -.25\">\n",
    "          <joint axis=\"0 1 0\" damping=\"4.5\" name=\"bshin\" pos=\"0 0 0\" range=\"-.785 .785\" stiffness=\"180\" type=\"hinge\"/>\n",
    "          <geom axisangle=\"0 1 0 -2.03\" name=\"bshin\" pos=\"-.14 0 -.07\" rgba=\"0.9 0.6 0.6 1\" size=\"0.046 .15\" type=\"capsule\"/>\n",
    "          <body name=\"bfoot\" pos=\"-.28 0 -.14\">\n",
    "            <joint axis=\"0 1 0\" damping=\"3\" name=\"bfoot\" pos=\"0 0 0\" range=\"-.4 .785\" stiffness=\"120\" type=\"hinge\"/>\n",
    "            <geom axisangle=\"0 1 0 -.27\" name=\"bfoot\" pos=\".03 0 -.097\" rgba=\"0.9 0.6 0.6 1\" size=\"0.046 .094\" type=\"capsule\"/>\n",
    "          </body>\n",
    "        </body>\n",
    "      </body>\n",
    "      <body name=\"fthigh\" pos=\".5 0 0\">\n",
    "        <joint axis=\"0 1 0\" damping=\"4.5\" name=\"fthigh\" pos=\"0 0 0\" range=\"-1 .7\" stiffness=\"180\" type=\"hinge\"/>\n",
    "        <geom axisangle=\"0 1 0 .52\" name=\"fthigh\" pos=\"-.07 0 -.12\" size=\"0.046 .133\" type=\"capsule\"/>\n",
    "        <body name=\"fshin\" pos=\"-.14 0 -.24\">\n",
    "          <joint axis=\"0 1 0\" damping=\"3\" name=\"fshin\" pos=\"0 0 0\" range=\"-1.2 .87\" stiffness=\"120\" type=\"hinge\"/>\n",
    "          <geom axisangle=\"0 1 0 -.6\" name=\"fshin\" pos=\".065 0 -.09\" rgba=\"0.9 0.6 0.6 1\" size=\"0.046 .106\" type=\"capsule\"/>\n",
    "          <body name=\"ffoot\" pos=\".13 0 -.18\">\n",
    "            <joint axis=\"0 1 0\" damping=\"1.5\" name=\"ffoot\" pos=\"0 0 0\" range=\"-.5 .5\" stiffness=\"60\" type=\"hinge\"/>\n",
    "            <geom axisangle=\"0 1 0 -.6\" name=\"ffoot\" pos=\".045 0 -.07\" rgba=\"0.9 0.6 0.6 1\" size=\"0.046 .07\" type=\"capsule\"/>\n",
    "          </body>\n",
    "        </body>\n",
    "      </body>\n",
    "    </body>\n",
    "  </worldbody>\n",
    "  <actuator>\n",
    "    <motor gear=\"120\" joint=\"bthigh\" name=\"bthigh\"/>\n",
    "    <motor gear=\"90\" joint=\"bshin\" name=\"bshin\"/>\n",
    "    <motor gear=\"60\" joint=\"bfoot\" name=\"bfoot\"/>\n",
    "    <motor gear=\"120\" joint=\"fthigh\" name=\"fthigh\"/>\n",
    "    <motor gear=\"60\" joint=\"fshin\" name=\"fshin\"/>\n",
    "    <motor gear=\"30\" joint=\"ffoot\" name=\"ffoot\"/>\n",
    "  </actuator>\n",
    "</mujoco>\"\"\""
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "78be6f1686c7a640"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class HalfCheetahEnv():\n",
    "  def __init__(\n",
    "      self,\n",
    "      frame_skip=5,\n",
    "      forward_reward_weight=1.0,\n",
    "      ctrl_cost_weight=0.1,\n",
    "      reset_noise_scale=0.1\n",
    "      ):\n",
    "\n",
    "    self.frame_skip = frame_skip\n",
    "    self.forward_reward_weight = forward_reward_weight\n",
    "    self.ctrl_cost_weight = ctrl_cost_weight\n",
    "    self.reset_noise_scale = reset_noise_scale\n",
    "\n",
    "    self.initialize_simulation()\n",
    "    self.init_qpos = self.data.qpos.ravel().copy()\n",
    "    self.init_qvel = self.data.qvel.ravel().copy()\n",
    "    self.dt = self.model.opt.timestep * self.frame_skip\n",
    "\n",
    "    self.observation_dim = 17\n",
    "    self.action_dim = 6\n",
    "    self.action_limit = 1.\n",
    "\n",
    "  def initialize_simulation(self):\n",
    "    self.model = mujoco.MjModel.from_xml_string(xml_string)\n",
    "    self.data = mujoco.MjData(self.model)\n",
    "    mujoco.mj_resetData(self.model, self.data)\n",
    "    self.renderer = mujoco.Renderer(self.model)\n",
    "\n",
    "  def reset_simulation(self):\n",
    "    mujoco.mj_resetData(self.model, self.data)\n",
    "\n",
    "  def step_mujoco_simulation(self, ctrl, n_frames):\n",
    "    self.data.ctrl[:] = ctrl\n",
    "    mujoco.mj_step(self.model, self.data, nstep=n_frames)\n",
    "    self.renderer.update_scene(self.data,0)\n",
    "\n",
    "  def set_state(self, qpos, qvel):\n",
    "    self.data.qpos[:] = np.copy(qpos)\n",
    "    self.data.qvel[:] = np.copy(qvel)\n",
    "    if self.model.na == 0:\n",
    "      self.data.act[:] = None\n",
    "    mujoco.mj_forward(self.model, self.data)\n",
    "\n",
    "  def sample_action(self):\n",
    "    return (2.*np.random.uniform(size=(self.action_dim,)) - 1)*self.action_limit\n",
    "\n",
    "  def step(self, action):\n",
    "    x_position_before = self.data.qpos[0]\n",
    "    self.step_mujoco_simulation(action, self.frame_skip)\n",
    "    x_position_after = self.data.qpos[0]\n",
    "    x_velocity = (x_position_after - x_position_before) / self.dt\n",
    "\n",
    "    # Rewards\n",
    "    ctrl_cost = self.ctrl_cost_weight * np.sum(np.square(action))\n",
    "    forward_reward = self.forward_reward_weight * x_velocity\n",
    "    observation = self.get_obs()\n",
    "    reward = forward_reward - ctrl_cost\n",
    "    terminated = False\n",
    "    info = {\n",
    "        \"x_position\": x_position_after,\n",
    "        \"x_velocity\": x_velocity,\n",
    "        \"reward_run\": forward_reward,\n",
    "        \"reward_ctrl\": -ctrl_cost,\n",
    "    }\n",
    "    return observation, reward, terminated, info\n",
    "\n",
    "  def get_obs(self):\n",
    "    position = self.data.qpos.flat.copy()\n",
    "    velocity = self.data.qvel.flat.copy()\n",
    "    position = position[1:]\n",
    "\n",
    "    observation = np.concatenate((position, velocity)).ravel()\n",
    "    return observation\n",
    "\n",
    "  def render(self):\n",
    "    return self.renderer.render()\n",
    "\n",
    "  def reset(self):\n",
    "    self.reset_simulation()\n",
    "    noise_low = -self.reset_noise_scale\n",
    "    noise_high = self.reset_noise_scale\n",
    "    qpos = self.init_qpos + np.random.uniform(\n",
    "        low=noise_low, high=noise_high, size=self.model.nq\n",
    "    )\n",
    "    qvel = (\n",
    "        self.init_qvel\n",
    "        + self.reset_noise_scale * np.random.standard_normal(self.model.nv)\n",
    "    )\n",
    "    self.set_state(qpos, qvel)\n",
    "    observation = self.get_obs()\n",
    "    return observation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "eef2c3f6b4e2be9b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f68730ef28b9d2eb"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Define the Buffer Class"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2a777f63041c38b8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, obs_dim, action_dim, buffer_size, device=\"cpu\"):\n",
    "        self._buffer_size = buffer_size\n",
    "        self._pointer = 0\n",
    "        self._size = 0\n",
    "\n",
    "        self._obses = torch.zeros((buffer_size, obs_dim), dtype=torch.float32, device=device)\n",
    "        self._actions = torch.zeros((buffer_size, action_dim), dtype=torch.float32, device=device)\n",
    "        self._rewards = torch.zeros((buffer_size, 1), dtype=torch.float32, device=device)\n",
    "        self._next_obses = torch.zeros((buffer_size, obs_dim), dtype=torch.float32, device=device)\n",
    "        self._dones = torch.zeros((buffer_size, 1), dtype=torch.float32, device=device)\n",
    "        self._device = device\n",
    "\n",
    "    def _to_tensor(self, data: np.ndarray) -> torch.Tensor:\n",
    "        return torch.tensor(data, dtype=torch.float32, device=self._device)\n",
    "\n",
    "    def load_dataset(self, dataset):\n",
    "        n_transitions = dataset[\"observations\"].shape[0]\n",
    "        self._obses[:n_transitions] = self._to_tensor(dataset[\"observations\"])\n",
    "        self._actions[:n_transitions] = self._to_tensor(dataset[\"actions\"])\n",
    "        self._rewards[:n_transitions] = self._to_tensor(dataset[\"rewards\"][..., None])\n",
    "        self._next_obses[:n_transitions] = self._to_tensor(dataset[\"next_observations\"])\n",
    "        self._dones[:n_transitions] = self._to_tensor(dataset[\"terminals\"][..., None])\n",
    "        self._size += n_transitions\n",
    "        self._pointer = min(self._size, n_transitions)\n",
    "        print(f\"Dataset size: {n_transitions}\")\n",
    "\n",
    "    def add_batch(self, observations, next_observations, actions, rewards, terminals):\n",
    "        batch_size = len(terminals)\n",
    "        if self._pointer + batch_size > self._buffer_size:\n",
    "            begin = self._pointer\n",
    "            end = self._buffer_size\n",
    "            first_add_size = end - begin\n",
    "            self._obses[begin:end] = self._to_tensor(observations[:first_add_size].copy())\n",
    "            self._next_obses[begin:end] = self._to_tensor(next_observations[:first_add_size].copy())\n",
    "            self._actions[begin:end] = self._to_tensor(actions[:first_add_size].copy())\n",
    "            self._rewards[begin:end] = self._to_tensor(rewards[:first_add_size].copy())\n",
    "            self._dones[begin:end] = self._to_tensor(terminals[:first_add_size].copy())\n",
    "\n",
    "            begin = 0\n",
    "            end = batch_size - first_add_size\n",
    "            self._obses[begin:end] = self._to_tensor(observations[first_add_size:].copy())\n",
    "            self._next_obses[begin:end] = self._to_tensor(next_observations[first_add_size:].copy())\n",
    "            self._actions[begin:end] = self._to_tensor(actions[first_add_size:].copy())\n",
    "            self._rewards[begin:end] = self._to_tensor(rewards[first_add_size:].copy())\n",
    "            self._dones[begin:end] = self._to_tensor(terminals[first_add_size:].copy())\n",
    "\n",
    "            self._pointer = end\n",
    "            self._size = min(self._size + batch_size, self._buffer_size)\n",
    "\n",
    "        else:\n",
    "            begin = self._pointer\n",
    "            end = self._pointer + batch_size\n",
    "            self._obses[begin:end] = self._to_tensor(observations.copy())\n",
    "            self._next_obses[begin:end] = self._to_tensor(next_observations.copy())\n",
    "            self._actions[begin:end] = self._to_tensor(actions.copy())\n",
    "            self._rewards[begin:end] = self._to_tensor(rewards.copy())\n",
    "            self._dones[begin:end] = self._to_tensor(terminals.copy())\n",
    "\n",
    "            self._pointer = end\n",
    "            self._size = min(self._size + batch_size, self._buffer_size)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        indices = np.random.randint(0, min(self._size, self._pointer), size=batch_size)\n",
    "        states = self._obses[indices]\n",
    "        actions = self._actions[indices]\n",
    "        rewards = self._rewards[indices]\n",
    "        next_states = self._next_obses[indices]\n",
    "        dones = self._dones[indices]\n",
    "        return [states, actions, rewards, next_states, dones]\n",
    "\n",
    "    def sample_all(self, batch_size):\n",
    "        num_batches = int((self._pointer+1)/batch_size)\n",
    "        indices = np.arange(self._pointer)\n",
    "        np.random.shuffle(indices)\n",
    "        for batch_id in range(num_batches):\n",
    "            batch_start = batch_id * batch_size\n",
    "            batch_end = min(self._pointer, (batch_id + 1) * batch_size)\n",
    "\n",
    "            states = self._obses[batch_start:batch_end]\n",
    "            actions = self._actions[batch_start:batch_end]\n",
    "            rewards = self._rewards[batch_start:batch_end]\n",
    "            next_states = self._next_obses[batch_start:batch_end]\n",
    "            dones = self._dones[batch_start:batch_end]\n",
    "            yield [states, actions, rewards, next_states, dones]\n",
    "\n",
    "    def normalize_states(self, eps = 1e-3, mean=None, std=None):\n",
    "        mean = self._obses.mean(0,keepdims=True)\n",
    "        std = self._obses.std(0,keepdims=True) + eps\n",
    "        self._obses = (self._obses - mean)/std\n",
    "        self._next_obses = (self._next_obses - mean)/std\n",
    "        return mean.cpu().data.numpy().flatten(), std.cpu().data.numpy().flatten()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "61f9bc12347e23d4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def get_dataset(env, num_trajs=100, max_ep_len = 1000, file_name='expert_dataset.pickle'):\n",
    "\n",
    "  if file_name is not None:\n",
    "    with open(file_name, 'rb') as f:\n",
    "      dataset = pickle.load(f)\n",
    "      ravg = dataset['ravg']\n",
    "      print(f\"Average return of Offline Dataset: {np.mean(ravg)}\")\n",
    "      return dataset\n",
    "\n",
    "  obs_dim = env.observation_dim\n",
    "  act_dim = env.action_dim\n",
    "\n",
    "  actions = []\n",
    "  observations = []\n",
    "  next_observations = []\n",
    "  rewards = []\n",
    "  terminals = []\n",
    "  ravg = []\n",
    "\n",
    "  o, ep_ret, ep_len = env.reset(), 0, 0\n",
    "  for t in range(num_trajs * max_ep_len):\n",
    "    a = env.sample_action() \n",
    "\n",
    "    o2, r, d, _ = env.step(a) \n",
    "    ep_ret += r\n",
    "    ep_len += 1\n",
    "    d = False if ep_len==max_ep_len else d\n",
    "\n",
    "    observations.append(o)\n",
    "    actions.append(a)\n",
    "    next_observations.append(o2)\n",
    "    rewards.append(r)\n",
    "    terminals.append(d)\n",
    "\n",
    "    o = o2\n",
    "    if d or (ep_len == max_ep_len):\n",
    "        ravg.append(ep_ret)\n",
    "        o, ep_ret, ep_len = env.reset(), 0, 0\n",
    "  print(f\"Average Return of Offline Dataset: {np.mean(ravg)}\")\n",
    "\n",
    "  observations = np.array(observations).astype(np.float32)\n",
    "  actions = np.array(actions).astype(np.float32)\n",
    "  next_observations = np.array(next_observations).astype(np.float32)\n",
    "  rewards = np.array(rewards).astype(np.float32)\n",
    "  terminals = np.array(terminals).astype(np.bool_)\n",
    "  return {\"observations\":observations,\"actions\":actions,\"next_observations\":next_observations,\"rewards\":rewards,\"terminals\":terminals}\n",
    "\n",
    "env = HalfCheetahEnv()\n",
    "dataset = get_dataset(env)\n",
    "\n",
    "obs_dim = env.observation_dim\n",
    "act_dim = env.action_dim\n",
    "act_lim = env.action_limit\n",
    "replay_buffer = ReplayBuffer(obs_dim, act_dim, 2000000, device)\n",
    "replay_buffer.load_dataset(dataset)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "73fb7d43e6981a7a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Define the Network"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ae04240fbb81bee8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def mlp(sizes, activation, output_activation=nn.Identity):\n",
    "    layers = []\n",
    "    for j in range(len(sizes)-1):\n",
    "        act = activation if j < len(sizes)-2 else output_activation\n",
    "        layers += [nn.Linear(sizes[j], sizes[j+1]), act()]\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "class Swish(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Swish, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x * torch.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "class EnsembleModel(nn.Module):\n",
    "    def __init__(self, obs_dim, act_dim, hidden_sizes, activation=Swish, output_activation=nn.Identity, reward_dim=1, ensemble_size=7, num_elite=5, decay_weights=None):\n",
    "        super(EnsembleModel, self).__init__()\n",
    "\n",
    "        self.out_dim = obs_dim + reward_dim\n",
    "\n",
    "        self.ensemble_models = [mlp([obs_dim + act_dim] + list(hidden_sizes) + [self.out_dim * 2], activation, output_activation) for _ in range(ensemble_size)]\n",
    "        for i in range(ensemble_size):\n",
    "            self.add_module(\"model_{}\".format(i), self.ensemble_models[i])\n",
    "\n",
    "        self.obs_dim = obs_dim\n",
    "        self.action_dim = act_dim\n",
    "        self.num_elite = num_elite\n",
    "        self.ensemble_size = ensemble_size\n",
    "        self.decay_weights = decay_weights\n",
    "        self.elite_model_idxes = torch.tensor([i for i in range(num_elite)])\n",
    "        self.max_logvar = nn.Parameter((torch.ones((1, self.out_dim)).float() / 2).to(device), requires_grad=True)\n",
    "        self.min_logvar = nn.Parameter((-torch.ones((1, self.out_dim)).float() * 10).to(device), requires_grad=True)\n",
    "        self.register_parameter(\"max_logvar\", self.max_logvar)\n",
    "        self.register_parameter(\"min_logvar\", self.min_logvar)\n",
    "\n",
    "    def predict(self, input):\n",
    "        # convert input to tensors\n",
    "        if type(input) != torch.Tensor:\n",
    "            if len(input.shape) == 1:\n",
    "                input = torch.FloatTensor([input]).to(device)\n",
    "            else:\n",
    "                input = torch.FloatTensor(input).to(device)\n",
    "\n",
    "        # predict\n",
    "        if len(input.shape) == 3:\n",
    "            model_outputs = [net(ip) for ip, net in zip(torch.unbind(input), self.ensemble_models)]\n",
    "        elif len(input.shape) == 2:\n",
    "            model_outputs = [net(input) for net in self.ensemble_models]\n",
    "        predictions = torch.stack(model_outputs)\n",
    "\n",
    "        mean = predictions[:, :, :self.out_dim]\n",
    "        logvar = predictions[:, :, self.out_dim:]\n",
    "        logvar = self.max_logvar - F.softplus(self.max_logvar - logvar)\n",
    "        logvar = self.min_logvar + F.softplus(logvar - self.min_logvar)\n",
    "\n",
    "        return mean, logvar\n",
    "\n",
    "    def get_decay_loss(self):\n",
    "        decay_losses = []\n",
    "        for model_net in self.ensemble_models:\n",
    "            curr_net_decay_losses = [decay_weight * torch.sum(torch.square(weight)) for decay_weight, weight in\n",
    "                                     zip(self.decay_weights, model_net.weights)]\n",
    "            decay_losses.append(torch.sum(torch.stack(curr_net_decay_losses)))\n",
    "        return torch.sum(torch.stack(decay_losses))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "39890f4dbad8c288"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Ensemble Dynamic Model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bc257eb7ee8b495b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ensemble_dynamic_model = EnsembleModel(obs_dim=obs_dim, act_dim=act_dim, hidden_sizes=[200, 200, 200, 200]).to(device)\n",
    "dynamics_lr = 1e-3\n",
    "ensemble_dynamic_model_optimizer = Adam(ensemble_dynamic_model.parameters(), dynamics_lr)\n",
    "\n",
    "data_mean, data_std = replay_buffer.normalize_states()\n",
    "batch = replay_buffer.sample(256)\n",
    "batch = [b.to(device) for b in batch]\n",
    "observations, actions, rewards, next_observations, dones = batch\n",
    "delta_observations = next_observations - observations\n",
    "groundtruths = torch.cat((delta_observations, rewards), dim=-1)\n",
    "\n",
    "model_input = torch.cat([observations, actions], dim=-1).to(device)\n",
    "predictions = ensemble_dynamic_model.predict(model_input)\n",
    "pred_means, pred_logvars = predictions\n",
    "train_mse_losses = torch.mean(torch.pow(pred_means - groundtruths, 2), dim=(1, 2))\n",
    "train_mse_loss = torch.sum(train_mse_losses)\n",
    "train_transition_loss = train_mse_loss\n",
    "train_transition_loss += 0.01 * torch.sum(ensemble_dynamic_model.max_logvar) - 0.01 * torch.sum(ensemble_dynamic_model.min_logvar)\n",
    "\n",
    "print(pred_means.shape, pred_logvars.shape, train_mse_losses.shape)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8a592c3502969fa9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Dynamic Model Update"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "219db214db970d4e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ensemble_dynamic_model_optimizer.zero_grad()\n",
    "train_transition_loss.backward()\n",
    "ensemble_dynamic_model_optimizer.step()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "57957ffe62206e89"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Evaluate of Validation data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c28d5831da10c0b9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "eval_mse_total_losses=np.zeros((ensemble_dynamic_model.ensemble_size,))\n",
    "for eval_batch in replay_buffer.sample_all(256):\n",
    "  eval_batch = [b.to(device) for b in eval_batch]\n",
    "  eval_observations, eval_actions, eval_rewards, eval_next_observations, eval_dones = eval_batch\n",
    "  eval_delta_observations = eval_next_observations - eval_observations\n",
    "  eval_groundtruths = torch.cat((eval_delta_observations, eval_rewards), dim=-1)\n",
    "  eval_model_input = torch.cat([eval_observations, eval_actions], dim=-1).to(device)\n",
    "  eval_predictions = ensemble_dynamic_model.predict(eval_model_input)\n",
    "  eval_pred_means, eval_pred_logvars = eval_predictions\n",
    "  eval_mse_losses = torch.mean(torch.pow(eval_pred_means - eval_groundtruths, 2), dim=(1, 2)).to('cpu').detach().numpy()\n",
    "  eval_mse_total_losses += eval_mse_losses\n",
    "print(eval_mse_total_losses)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dabe4f1d925ff53e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Save the model of the best evaluation loss"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "684d65b2715b5cdf"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "best_snapshot_losses=np.full((ensemble_dynamic_model.ensemble_size,), 1e10)\n",
    "model_best_snapshots=[deepcopy(ensemble_dynamic_model.ensemble_models[idx].state_dict()) for idx in range(ensemble_dynamic_model.ensemble_size)]\n",
    "\n",
    "updated = False\n",
    "for i in range(len(eval_mse_total_losses)):\n",
    "  current_loss = eval_mse_total_losses[i]\n",
    "  best_loss = best_snapshot_losses[i]\n",
    "  improvement = (best_loss - current_loss) / best_loss\n",
    "  if improvement > 0.01:\n",
    "    best_snapshot_losses[i] = current_loss\n",
    "    model_best_snapshots[i] = deepcopy(ensemble_dynamic_model.ensemble_models[i].state_dict())\n",
    "    updated = True"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a75c4f5e88989f23"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Load the best model parameter"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ba863019fe1c0b4c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for i in range(ensemble_dynamic_model.ensemble_size):\n",
    "  ensemble_dynamic_model.ensemble_models[i].load_state_dict(model_best_snapshots[i])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "76d9cfae24828540"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Define the Network"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d75b9e61bd95254b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "LOG_STD_MAX = 2\n",
    "LOG_STD_MIN = -20\n",
    "MEAN_MIN = -9.0\n",
    "MEAN_MAX = 9.0\n",
    "\n",
    "\n",
    "class SquashedGaussianMLPActor(nn.Module):\n",
    "    def __init__(self, obs_dim, act_dim, hidden_sizes, activation, act_limit):\n",
    "        super().__init__()\n",
    "        self.net = mlp([obs_dim] + list(hidden_sizes), activation, activation)\n",
    "        self.mu_layer = nn.Linear(hidden_sizes[-1], act_dim)\n",
    "        self.log_std_layer = nn.Linear(hidden_sizes[-1], act_dim)\n",
    "        self.act_limit = act_limit\n",
    "\n",
    "    def log_prob(self, obs, actions):\n",
    "        net_out = self.net(obs)\n",
    "        mu = self.mu_layer(net_out)\n",
    "        log_std = self.log_std_layer(net_out)\n",
    "        log_std = torch.clamp(log_std, LOG_STD_MIN, LOG_STD_MAX)\n",
    "        std = torch.exp(log_std)\n",
    "\n",
    "        pi_distribution = Normal(mu, std)\n",
    "        log_prob = pi_distribution.log_prob(actions).sum(axis=-1)\n",
    "        log_prob -= (2*(np.log(2) - actions - F.softplus(-2*actions))).sum(axis=1)\n",
    "        return log_prob.sum(-1)\n",
    "\n",
    "    def forward(self, obs, deterministic=False, with_logprob=True):\n",
    "        net_out = self.net(obs)\n",
    "        mu = self.mu_layer(net_out)\n",
    "        log_std = self.log_std_layer(net_out)\n",
    "        log_std = torch.clamp(log_std, LOG_STD_MIN, LOG_STD_MAX)\n",
    "        std = torch.exp(log_std)\n",
    "\n",
    "        pi_distribution = Normal(mu, std)\n",
    "        if deterministic:\n",
    "            pi_action = mu\n",
    "        else:\n",
    "            pi_action = pi_distribution.rsample()\n",
    "\n",
    "        if with_logprob:\n",
    "            logp_pi = pi_distribution.log_prob(pi_action).sum(axis=-1)\n",
    "            logp_pi -= (2*(np.log(2) - pi_action - F.softplus(-2*pi_action))).sum(axis=1)\n",
    "        else:\n",
    "            logp_pi = None\n",
    "\n",
    "        pi_action = torch.tanh(pi_action)\n",
    "        pi_action = self.act_limit * pi_action\n",
    "\n",
    "        return pi_action, logp_pi\n",
    "\n",
    "class MLPQFunction(nn.Module):\n",
    "    def __init__(self, obs_dim, act_dim, hidden_sizes, activation):\n",
    "        super().__init__()\n",
    "        self.q = mlp([obs_dim + act_dim] + list(hidden_sizes) + [1], activation)\n",
    "\n",
    "    def forward(self, obs, act):\n",
    "        q = self.q(torch.cat([obs, act], dim=-1))\n",
    "        return torch.squeeze(q, -1) # Critical to ensure q has right shape."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5c4ce8f424f34be6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Define Network and Optimizer"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e7a7e85ad5447522"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "hidden_sizes = [256, 256, 256] \n",
    "activation = nn.ReLU \n",
    "\n",
    "soft_update_tau = 5e-3 \n",
    "policy_lr=1e-4 \n",
    "qf_lr=3e-4 \n",
    "target_entropy = - act_dim \n",
    "\n",
    "qf1 = MLPQFunction(obs_dim, act_dim, hidden_sizes, activation).to(device)\n",
    "qf2 = MLPQFunction(obs_dim, act_dim, hidden_sizes, activation).to(device)\n",
    "target_qf1 = deepcopy(qf1).to(device)\n",
    "target_qf2 = deepcopy(qf2).to(device)\n",
    "policy = SquashedGaussianMLPActor(obs_dim, act_dim, hidden_sizes, activation, act_lim).to(device)\n",
    "log_alpha = torch.zeros(1, requires_grad=True, device=device)\n",
    "\n",
    "alpha_optimizer = Adam([log_alpha], lr=policy_lr)\n",
    "policy_optimizer = Adam(policy.parameters(), lr=policy_lr)\n",
    "qf1_optimizer = Adam(qf1.parameters(), lr=qf_lr)\n",
    "qf2_optimizer = Adam(qf2.parameters(), lr=qf_lr)\n",
    "\n",
    "rollout_freq=2\n",
    "rollout_batch_size=1000\n",
    "rollout_length=5\n",
    "\n",
    "model_buffer = ReplayBuffer(\n",
    "    obs_dim,\n",
    "    act_dim,\n",
    "    1000000,\n",
    "    device)\n",
    "\n",
    "init_transitions = replay_buffer.sample(rollout_batch_size)\n",
    "# rollout\n",
    "observations = init_transitions[0]\n",
    "for _ in range(rollout_length):\n",
    "    actions, _ = policy(observations)\n",
    "    model_input = torch.cat([observations, actions], dim=-1).to(device)\n",
    "    pred_diff_means, pred_diff_logvars = ensemble_dynamic_model.predict(model_input)\n",
    "    observations = observations.detach().cpu().numpy()\n",
    "    actions = actions.detach().cpu().numpy()\n",
    "    ensemble_model_stds = pred_diff_logvars.exp().sqrt().detach().cpu().numpy()\n",
    "    pred_diff_means = pred_diff_means.detach().cpu().numpy()\n",
    "    pred_diff_means = pred_diff_means + np.random.normal(size=pred_diff_means.shape) * ensemble_model_stds\n",
    "\n",
    "    num_models, batch_size, _ = pred_diff_means.shape\n",
    "    model_idxes = np.random.choice(ensemble_dynamic_model.elite_model_idxes, size=batch_size)\n",
    "    batch_idxes = np.arange(0, batch_size)\n",
    "    pred_diff_samples = pred_diff_means[model_idxes, batch_idxes]\n",
    "\n",
    "    next_observations, rewards = pred_diff_samples[:, :-1] + observations, pred_diff_samples[:, [-1]]\n",
    "    penalty = np.amax(np.linalg.norm(ensemble_model_stds, axis=2), axis=0)\n",
    "    penalty = np.expand_dims(penalty, 1)\n",
    "    rewards = rewards - 5e-1 * penalty\n",
    "\n",
    "    terminals = np.full((batch_size,1),False)\n",
    "    model_buffer.add_batch(observations, next_observations, actions, rewards, terminals)\n",
    "    observations = torch.tensor(next_observations, dtype=torch.float32, device=device)\n",
    "    \n",
    "mixing_ratio = 0.1\n",
    "batch_size = 256\n",
    "\n",
    "replay_batch_size = int(batch_size*(1-mixing_ratio))\n",
    "model_batch_size = batch_size - replay_batch_size"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5b3c2cdce1dca2ea"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Batch sampling"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "379a7880d2f64cea"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "replay_batch = replay_buffer.sample(replay_batch_size)\n",
    "model_batch = model_buffer.sample(model_batch_size)\n",
    "observations, actions, rewards, next_observations, dones = [torch.concat([r_b, m_b]) for r_b, m_b in zip(replay_batch, model_batch)]\n",
    "\n",
    "discount = 0.99\n",
    "\n",
    "new_actions, log_pi = policy(observations)\n",
    "alpha_loss = -(log_alpha * (log_pi + target_entropy).detach()).mean()\n",
    "alpha = log_alpha.exp()\n",
    "\n",
    "q1_predicted = qf1(observations, actions)\n",
    "q2_predicted = qf2(observations, actions)\n",
    "\n",
    "new_next_actions, next_log_pi = policy(next_observations)\n",
    "target_q_values = torch.min(target_qf1(next_observations, new_next_actions),target_qf2(next_observations, new_next_actions))\n",
    "target_q_values = target_q_values - alpha * next_log_pi\n",
    "target_q_values = target_q_values.unsqueeze(-1)\n",
    "\n",
    "td_target = rewards + (1.0 - dones) * discount * target_q_values\n",
    "td_target = td_target.squeeze(-1)\n",
    "\n",
    "qf1_loss = F.mse_loss(q1_predicted, td_target.detach())\n",
    "qf2_loss = F.mse_loss(q2_predicted, td_target.detach())\n",
    "qf_loss = qf1_loss + qf2_loss\n",
    "\n",
    "q_new_actions = torch.min(qf1(observations, new_actions), qf2(observations, new_actions))\n",
    "policy_loss = (alpha * log_pi - q_new_actions).mean()\n",
    "\n",
    "alpha_optimizer.zero_grad()\n",
    "alpha_loss.backward()\n",
    "alpha_optimizer.step()\n",
    "\n",
    "policy_optimizer.zero_grad()\n",
    "policy_loss.backward()\n",
    "policy_optimizer.step()\n",
    "\n",
    "qf1_optimizer.zero_grad()\n",
    "qf2_optimizer.zero_grad()\n",
    "qf_loss.backward(retain_graph=True)\n",
    "qf1_optimizer.step()\n",
    "qf2_optimizer.step()\n",
    "\n",
    "def soft_update(target, source, tau):\n",
    "    for target_param, source_param in zip(target.parameters(), source.parameters()):\n",
    "        target_param.data.copy_((1 - tau) * target_param.data + tau * source_param.data)\n",
    "\n",
    "soft_update(target_qf1, qf1, soft_update_tau)\n",
    "soft_update(target_qf2, qf2, soft_update_tau)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e3a4795806d2dad3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Whole code of Model-based offline Policy Optimization"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "84437aa9e1331460"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def mopo(env_fn, max_iterations4dynamic_model=10000, max_total_steps=50000, buffer_size=1000000,\n",
    "         dynamics_lr = 1e-3, batch_size=256, hidden_sizes = [256, 256, 256], activation = nn.ReLU,\n",
    "         soft_update_tau = 5e-3, policy_lr=1e-4, qf_lr=3e-4, rollout_freq=2,\n",
    "         rollout_batch_size=1000, rollout_length=5, mixing_ratio = 0.1, discount = 0.99):\n",
    "  env = env_fn()\n",
    "  dataset = get_dataset(env, file_name=\"expert_dataset.pickle\")\n",
    "\n",
    "  obs_dim = env.observation_dim\n",
    "  act_dim = env.action_dim\n",
    "  act_lim = env.action_limit\n",
    "  replay_buffer = ReplayBuffer(obs_dim, act_dim, buffer_size, device)\n",
    "  replay_buffer.load_dataset(dataset)\n",
    "  data_mean, data_std = replay_buffer.normalize_states()\n",
    "\n",
    "  ensemble_dynamic_model = EnsembleModel(obs_dim=obs_dim, act_dim=act_dim, hidden_sizes=[200, 200, 200, 200]).to(device)\n",
    "\n",
    "  ensemble_dynamic_model_optimizer = Adam(ensemble_dynamic_model.parameters(), dynamics_lr)\n",
    "  best_snapshot_losses=np.full((ensemble_dynamic_model.ensemble_size,), 1e10)\n",
    "  model_best_snapshots=[deepcopy(ensemble_dynamic_model.ensemble_models[idx].state_dict()) for idx in range(ensemble_dynamic_model.ensemble_size)]\n",
    "\n",
    "  for t in range(max_iterations4dynamic_model):\n",
    "    batch = replay_buffer.sample(batch_size)\n",
    "    batch = [b.to(device) for b in batch]\n",
    "    observations, actions, rewards, next_observations, dones = batch\n",
    "    delta_observations = next_observations - observations\n",
    "    groundtruths = torch.cat((delta_observations, rewards), dim=-1)\n",
    "\n",
    "    model_input = torch.cat([observations, actions], dim=-1).to(device)\n",
    "    predictions = ensemble_dynamic_model.predict(model_input)\n",
    "    pred_means, pred_logvars = predictions\n",
    "    train_mse_losses = torch.mean(torch.pow(pred_means - groundtruths, 2), dim=(1, 2))\n",
    "    train_mse_loss = torch.sum(train_mse_losses)\n",
    "    train_transition_loss = train_mse_loss\n",
    "    train_transition_loss += 0.01 * torch.sum(ensemble_dynamic_model.max_logvar) - 0.01 * torch.sum(ensemble_dynamic_model.min_logvar)\n",
    "\n",
    "    ensemble_dynamic_model_optimizer.zero_grad()\n",
    "    train_transition_loss.backward()\n",
    "    ensemble_dynamic_model_optimizer.step()\n",
    "\n",
    "    if (t%5000)==0:\n",
    "      eval_mse_total_losses=np.zeros((ensemble_dynamic_model.ensemble_size,))\n",
    "      for eval_batch in replay_buffer.sample_all(batch_size):\n",
    "        eval_batch = [b.to(device) for b in eval_batch]\n",
    "        eval_observations, eval_actions, eval_rewards, eval_next_observations, eval_dones = eval_batch\n",
    "        eval_delta_observations = eval_next_observations - eval_observations\n",
    "        eval_groundtruths = torch.cat((eval_delta_observations, eval_rewards), dim=-1)\n",
    "        eval_model_input = torch.cat([eval_observations, eval_actions], dim=-1).to(device)\n",
    "        eval_predictions = ensemble_dynamic_model.predict(eval_model_input)\n",
    "        eval_pred_means, eval_pred_logvars = eval_predictions\n",
    "        eval_mse_losses = torch.mean(torch.pow(eval_pred_means - eval_groundtruths, 2), dim=(1, 2)).to('cpu').detach().numpy()\n",
    "        eval_mse_total_losses += eval_mse_losses\n",
    "\n",
    "      updated = False\n",
    "      for i in range(len(eval_mse_total_losses)):\n",
    "        current_loss = eval_mse_total_losses[i]\n",
    "        best_loss = best_snapshot_losses[i]\n",
    "        improvement = (best_loss - current_loss) / best_loss\n",
    "        if improvement > 0.01:\n",
    "          best_snapshot_losses[i] = current_loss\n",
    "          model_best_snapshots[i] = deepcopy(ensemble_dynamic_model.ensemble_models[i].state_dict())\n",
    "          updated = True\n",
    "          print(f'{i}th model is updated!')\n",
    "      if updated:\n",
    "        print(f'[{t}]Dynamic model evaluation: {eval_mse_total_losses}')\n",
    "\n",
    "  for i in range(ensemble_dynamic_model.ensemble_size):\n",
    "    ensemble_dynamic_model.ensemble_models[i].load_state_dict(model_best_snapshots[i])\n",
    "\n",
    "  target_entropy = - act_dim \n",
    "\n",
    "  qf1 = MLPQFunction(obs_dim, act_dim, hidden_sizes, activation).to(device)\n",
    "  qf2 = MLPQFunction(obs_dim, act_dim, hidden_sizes, activation).to(device)\n",
    "  target_qf1 = deepcopy(qf1).to(device)\n",
    "  target_qf2 = deepcopy(qf2).to(device)\n",
    "  policy = SquashedGaussianMLPActor(obs_dim, act_dim, hidden_sizes, activation, act_lim).to(device)\n",
    "  log_alpha = torch.zeros(1, requires_grad=True, device=device)\n",
    "\n",
    "  alpha_optimizer = Adam([log_alpha], lr=policy_lr)\n",
    "  policy_optimizer = Adam(policy.parameters(), lr=policy_lr)\n",
    "  qf1_optimizer = Adam(qf1.parameters(), lr=qf_lr)\n",
    "  qf2_optimizer = Adam(qf2.parameters(), lr=qf_lr)\n",
    "\n",
    "  model_buffer = ReplayBuffer(obs_dim, act_dim, buffer_size, device)\n",
    "\n",
    "  print('Offline RL start')\n",
    "  replay_batch_size = int(batch_size*(1-mixing_ratio))\n",
    "  model_batch_size = batch_size - replay_batch_size\n",
    "\n",
    "  for t in range(max_total_steps):\n",
    "    if (t%rollout_freq)==0:\n",
    "      init_transitions = replay_buffer.sample(rollout_batch_size)\n",
    "      # rollout\n",
    "      observations = init_transitions[0]\n",
    "      for _ in range(rollout_length):\n",
    "          actions, _ = policy(observations)\n",
    "          model_input = torch.cat([observations, actions], dim=-1).to(device)\n",
    "          pred_diff_means, pred_diff_logvars = ensemble_dynamic_model.predict(model_input)\n",
    "          observations = observations.detach().cpu().numpy()\n",
    "          actions = actions.detach().cpu().numpy()\n",
    "          ensemble_model_stds = pred_diff_logvars.exp().sqrt().detach().cpu().numpy()\n",
    "          pred_diff_means = pred_diff_means.detach().cpu().numpy()\n",
    "          pred_diff_means = pred_diff_means + np.random.normal(size=pred_diff_means.shape) * ensemble_model_stds\n",
    "\n",
    "          num_models, batch_size, _ = pred_diff_means.shape\n",
    "          model_idxes = np.random.choice(ensemble_dynamic_model.elite_model_idxes, size=batch_size)\n",
    "          batch_idxes = np.arange(0, batch_size)\n",
    "          pred_diff_samples = pred_diff_means[model_idxes, batch_idxes]\n",
    "\n",
    "          next_observations, rewards = pred_diff_samples[:, :-1] + observations, pred_diff_samples[:, [-1]]\n",
    "          penalty = np.amax(np.linalg.norm(ensemble_model_stds, axis=2), axis=0)\n",
    "          penalty = np.expand_dims(penalty, 1)\n",
    "          rewards = rewards - 5e-1 * penalty\n",
    "\n",
    "          terminals = np.full((batch_size,1),False)\n",
    "          model_buffer.add_batch(observations, next_observations, actions, rewards, terminals)\n",
    "          observations = torch.tensor(next_observations, dtype=torch.float32, device=device)\n",
    "\n",
    "    replay_batch = replay_buffer.sample(replay_batch_size)\n",
    "    model_batch = model_buffer.sample(model_batch_size)\n",
    "\n",
    "    observations, actions, rewards, next_observations, dones = [torch.concat([r_b, m_b]) for r_b, m_b in zip(replay_batch, model_batch)]\n",
    "\n",
    "    new_actions, log_pi = policy(observations)\n",
    "    alpha_loss = -(log_alpha * (log_pi + target_entropy).detach()).mean()\n",
    "    alpha = log_alpha.exp()\n",
    "\n",
    "    q1_predicted = qf1(observations, actions)\n",
    "    q2_predicted = qf2(observations, actions)\n",
    "\n",
    "    new_next_actions, next_log_pi = policy(next_observations)\n",
    "    target_q_values = torch.min(target_qf1(next_observations, new_next_actions),target_qf2(next_observations, new_next_actions))\n",
    "    target_q_values = target_q_values - alpha * next_log_pi\n",
    "    target_q_values = target_q_values.unsqueeze(-1)\n",
    "\n",
    "    td_target = rewards + (1.0 - dones) * discount * target_q_values\n",
    "    td_target = td_target.squeeze(-1)\n",
    "\n",
    "    qf1_loss = F.mse_loss(q1_predicted, td_target.detach())\n",
    "    qf2_loss = F.mse_loss(q2_predicted, td_target.detach())\n",
    "    qf_loss = qf1_loss + qf2_loss\n",
    "\n",
    "    q_new_actions = torch.min(\n",
    "        qf1(observations, new_actions),\n",
    "        qf2(observations, new_actions),\n",
    "    )\n",
    "    policy_loss = (alpha * log_pi - q_new_actions).mean()\n",
    "\n",
    "    alpha_optimizer.zero_grad()\n",
    "    alpha_loss.backward()\n",
    "    alpha_optimizer.step()\n",
    "\n",
    "    policy_optimizer.zero_grad()\n",
    "    policy_loss.backward()\n",
    "    policy_optimizer.step()\n",
    "\n",
    "    qf1_optimizer.zero_grad()\n",
    "    qf2_optimizer.zero_grad()\n",
    "    qf_loss.backward(retain_graph=True)\n",
    "    qf1_optimizer.step()\n",
    "    qf2_optimizer.step()\n",
    "\n",
    "    soft_update(target_qf1, qf1, soft_update_tau)\n",
    "    soft_update(target_qf2, qf2, soft_update_tau)\n",
    "\n",
    "    if (t%4000)==0:\n",
    "      log_dict = dict(qf1_loss=qf1_loss.item(),\n",
    "                      qf2_loss=qf2_loss.item(),\n",
    "                      alpha_loss=alpha_loss.item(),\n",
    "                      policy_loss=policy_loss.item())\n",
    "\n",
    "      for keys, values in log_dict.items():\n",
    "          print(f'{keys}:{values:8.2f}',end=\", \")\n",
    "\n",
    "      avg_ret = []\n",
    "      for _ in range(10):\n",
    "        obs = env.reset()\n",
    "        ret = 0\n",
    "        for _t in range(1000):\n",
    "          obs = (obs - data_mean)/data_std\n",
    "          with torch.no_grad():\n",
    "            obs = torch.as_tensor(obs, dtype=torch.float32,device=device)\n",
    "            action, _ = policy(obs, deterministic=True, with_logprob=False)\n",
    "            action = action.to('cpu').numpy()\n",
    "          obs, reward, terminated, info = env.step(action)\n",
    "          ret += reward\n",
    "        avg_ret.append(ret)\n",
    "      print(f'Test Return:{np.mean(avg_ret):8.2f}')\n",
    "\n",
    "  return policy"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "45c6bdee03f22da"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "policy = mopo(HalfCheetahEnv)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "811311e692c562be"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
